<!DOCTYPE html
    PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN' 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Computer Graphics - Report of the project</title>

    <link href="resources/bootstrap.min.css" rel="stylesheet">
    <link href="resources/offcanvas.css" rel="stylesheet">
    <link href="resources/custom2014.css" rel="stylesheet">
    <link href="resources/twentytwenty.css" rel="stylesheet" type="text/css" />
</head>

<body>

    <div class="container headerBar">
        <h1>Computer Graphics 2022 - Project Report - François Costa & Andrew Dobis</h1>
    </div>

    <div class="container contentWrapper">
        <div class="pageContent">

            <h1>The scene</h1>

            <div class="twentytwenty-container">
                <img src="../scenes/project/final_image/final.png" alt="Scene" class="img-responsive">
            </div> <br>

            <h3>Images used as inspiration</h3>

            <div id="img-container">
                <img src="images/image.jpg" height="200">
                <img src="images/image2.jpeg" height="200">
                <img src="images/africa.jpg" height="200">
                <img src="images/asia.jpg" height="200">
                <img src="images/europe.jpeg" height="200">
                <img src="images/southamerica.jpeg" height="200">
                <img src="images/us.jpg" height="200">
            </div>

            <h3>Description of our image</h3>

            <p>
                The goal of our scene is to represent several landscapes within various spheres encased in a zoo-like
                presentative box. Each sphere contains the same terrain, shaded with a different bsdf. We chose not to
                include all of our features in this final scene, as we felt that it ended up being visually overwhelming
                for the viewer. Our sole light-source in this scene is a relatively simply hdri of the sky implemented
                using a strengthened environment map. We rendered our scene using 256 samples per pixel. As a
                post-processing effect, we used our denoiser in order to reduce the artifacts caused by the moderate
                sample count. We must note however that some artifacts still remain due to the chromatic aberration
                effect added by our advanced camera model.
            </p>

            <h1>Features</h1>
            <p>
                <strong>François:</strong>
            <ul>
                <li>NL-means Denoising with Pixel Variance Estimates</li>
                <li>Disney DSDF</li>
                <li>Spotlight Emitter</li>
                <li>Homogeneous Participating Media (Volumetric Path Tracing)</li>
                <li>Windowed Synch Reconstruction Filter</li>
                <li>Rendering on the Euler Cluster</li>
            </ul>
            <strong>Andrew:</strong>
            <ul>
                <li>Advanced Camera Model</li>
                <li>Environment Map Emitter</li>
                <li>Image Texture</li>
                <li>Normal Map</li>
                <li>Modeling Meshes</li>
                <li>Procedural Volumes (Perlin Noise)</li>
                <li><s>Emissive Participating Media</s> (abandoned due to time constraints)</li>
            </ul>
            In the following paragraphs, we will detail how each feature was implemented, followed by a feature-specific
            validation for each case.
            </p>
            <!-- ================================================================= -->

            <h2>Denoiser</h2>


            <p>
                <strong>Files Changed:</strong>
            <ul>
                <li><code>denoiser.py</code></li>
                <li><code>render.cpp</code></li>
            </ul>

            <h3>Description</h3>

            <p>The denoiser is clearly a very cool feature to program. At first, we had to
                modify the raytracer so that it also returns the variance per pixel. Indeed at the time of the
                generation of the scene, some pixels are much more "uniform" than others. The generation of
                image with the variance per pixel required a small adjustment in the ray tracer, but nothing very
                complicated. We simply had to add a second bitmap object and calculate the
                variance formula.</p>
            <p>
                The second part is to implement a denoiser that reads first the noisy output image
                and the variance per pixel. The implementation of this algorithm is in python and is located in a
                separate folder folder that I named denoiser. The choice was made for the fast version with
                convolutions.
            </p>

            <h3> Technical details : </h3>

            <p> Before writing the actualy python code for the denoiser, we need to render output two images. To render
                the image with variance per pixel, I modified the file render.cpp. Then I can compute the bitmap
                pixelVarianceEstimates using the two bitmaps sumBitmap and sum2Bitmap. The last step is to save the
                bitmap and we are done with this part.</p>

            <p>
                The second part is the python code. The denoiser can be splitted from the nori code, because it takes
                only two images as input and return return a denoised images. This is post-processing step in the
                rendering pipeline. The actual implementation was a copy paste of the pseudo code in the slides. The
                missing functions were created inside of python. The librairies numpy, scipy and opencv helped me to
                realize this denoiser. Some functions like
                circular shift and d2 had to be written by hand. OpenCV is mainly used for the I/O part of the
                part of the program. The program takes two mandatory arguments as input and displays three images,
                before save the denoised image for comparison.
            </p>

            <h3> Validation </h3>


            <div class="twentytwenty-container">
                <img src="../scenes/project/denoiser/final.png" alt="Noisy image" class="img-responsive">
                <img src="../scenes/project/denoiser/final_denoised.png" alt="Denoised image" class="img-responsive">
            </div> <br>
            <div class="twentytwenty-container">
                <img src="../scenes/project/denoiser/image.png" alt="Noisy image" class="img-responsive">
                <img src="../scenes/project/denoiser/denoised.png" alt="Denoised image" class="img-responsive">
            </div> <br>
            <div class="twentytwenty-container">
                <img src="../scenes/project/denoiser/image2.png" alt="Noisy image" class="img-responsive">
                <img src="../scenes/project/denoiser/denoised2.png" alt="Denoised image" class="img-responsive">
            </div> <br>
            <!-- ================================================================= -->

            <h2>Disney BRDF</h2>

            <p>
                <strong>Files Changed:</strong>
            <ul>
                <li><code>disney.cpp</code></li>
                <li><code>warp.cpp</code></li>
            </ul>

            <h3>Description</h3>

            <p>When I told my small cousin what I was doing, she had stars in her eyes.
                When I first read the Disney BRDF paper, I had a few tears in the eyes instead. Joking aside, this
                function was a real challenge to implement even though the final code looks very much like an
                microfacet function. For my part I implemented almost all the parameters except clearcoat and
                anisotropic. The implementation of the disney BRDF function is similar to the microfacet function, a
                implementation in three parts. For the implementation, I took inspiration from the following resources
            </p>

            <ul>
                <li><a>https://media.disneyanimation.com/uploads/production/publication_asset/48/asset/s2012_pbs_disney_brdf_notes_v3.pdf</a>
                </li>
                <li><a>https://github.com/wdas/brdf/blob/main/src/brdfs/disney.brdf</a></li>
                <li><a>https://schuttejoe.github.io/post/disneybsdf/</a></li>
            </ul>

            <h3> Technical details : </h3>

            <p>For the implementation of the brdf I created the single file disney.cpp. It contains the whole
                implementation. I also added GTR1 and GTR2 in the function warp.cpp.</p>

            <p>
                The first function I implemented is the evaluate function. Given a BSDFQueryRecord, the function returns
                a Color3f object. The first part of the function is a simple list of multiple easy dot computations to
                retrive different values for the next parts. Then we need to evaluate different Colors, namely
                Ctint,CtintMix, Cspec, Csheen. After we compute all values, we can start computing the differents
                components. At the end we combine the differentes components together.
                The first component is the diffuse part. The second is the specular part and the last one is the sheen
                part. For the specular part we need to sample the distribution function GTR2. I wrote the utility
                function in the file warp.cpp.</p>

            <p>
                The second function implemented is the sample function. The objective of this sample is to return a
                the value evaluated by the eval function, divided by the pdf and the cosine and return a new line
                in order to continue the recursive evaluation by means of this famous Russian roulette. In theory a
                uniform sampling would work without too much trouble with enough samples because it is not biased. In
                order to make the rendering look like better, importance sampling is a better choice.
                In my implementation I decided to divide the sampling in two parts. In the first part,
                we look if sample.x() is smaller than the metal factor or if it is not the case. In the first
                first case we do a CosineHemisphere sampling. In the second case, we sample the function
                squareToGTR2. The returned vector is considered as the normal vector and we make a reflection of the
                incoming vector on this new "normal" vector. I decided in this case not to take into account the sheen
                the sheen parameter in the sampling. Indeed this parameter has a relatively low value and
                I have seen several articles / implementations leaving it out. This allows to have a code base
                easier to read and debugging is also simplified.</p>

            <p>
                The last function to implement is the pdf. This function was not very difficult to
                to implement. The implementation of the pdf can be done exactly as in the case of the
                microfacet model. </p>


            <h3> Validation </h3>

            <p>You have also other image available in the folder scenes/project/disney/images</p>

            <div class="twentytwenty-container"><img src="../scenes/project/disney/test_gtr1.png"
                    alt="Test results - GTR1" />
            </div><br> <br> <br> <br>
            <div class="twentytwenty-container"><img src="../scenes/project/disney/test_gtr2.png"
                    alt="Test results - GTR2" />
            </div> <br> <br> <br> <br>

            <div class="twentytwenty-container">
                <img src="../scenes/project/disney/cbox_path_mis.png" alt="Disney BRDF" class="img-responsive">
            </div> <br>

            <div class="twentytwenty-container">
                <img src="../scenes/project/disney/cbox_path_mis_specular_no_rough.png" alt="Specular but not rough"
                    class="img-responsive">
                <img src="../scenes/project/disney/cbox_specular_10.png" alt="Specular & rough" class="img-responsive">
            </div> <br>

            <div class="twentytwenty-container">
                <img src="../scenes/project/disney/metal_03.png" alt="Metallic 0.3"
                    class="img-responsive">
                <img src="../scenes/project/disney/metal_10.png" alt="Metallic 1.0" class="img-responsive">
            </div> <br>
            <!-- ================================================================= -->

            <h2>Spotlight</h2>

            <p>
                <strong>Files Changed:</strong>
            <ul>
                <li><code>spotlight.cpp</code></li>
            </ul>


            <h3>Description</h3>

            <p>The third and last 5-point feature from me is a spotlight relatively similar to the point light. The
                difference between a pointlight and a spotlight is that the second one does not illuminate "all" the
                sphere but only a specific part of it near a direction angle.
            </p>

            <h3> Technical details : </h3>

            <p> The starting code is the one of my point light. I copied this code into the file spotlight.cpp. For the
                implementation of a spotlight, there are 3
                additional parameters. First of all, a spotlight has a vector which serves as the main direction, i.e.
                direction, which is the center of the spotlight. The next two parameters determine in a way the "size of
                the spolight". The first parameter cosFalloffStart is used to determine the size of the first cone where
                the lighting is almost complete. The second parameter cosTotalWidth draws a second cone that starts from
                the angle of the first parameter and ends at the angle of the second second parameter and the brightness
                will decrease linearly. The difference with the point light is that we add the function fallOff. This
                part of the functions attenuates the lights depending on the input angle.The details of the
                implementation are
                again strongly inspired by the PBR book. The code is similar to that of the pointlight. </p>

            <h3> Validation </h3>

            <div class="twentytwenty-container">
                <img src="../scenes/project/spotlight/sphere-texture.png" alt="Spotlight" class="img-responsive">
            </div> <br>

            <!-- ================================================================= -->

            <h2>Volumetric path tracing</h2>


            <p>
                <strong>Files Changed:</strong>
            <ul>
                <li><code>volumetric.cpp</code></li>
                <li><code>phasefunction.cpp</code></li>
                <li><code>medium.cpp</code></li>
            </ul>


            <h3>Description</h3>

            <p>
                This feature is probably one of the most satisfying of the project to see once it works.
                The implementation was particularly difficult, especially the part with the integrator which took me a
                lot of time and energy. </p>

            <h3> Technical details : </h3>

            <p> To implement a volumetric path tracer, I had to implement three main components. In the first step, we
                have a medium. In a second step, there is a a phase function. Once these two functions are implemented,
                we can move to the last function which is the function which is the integrator and therefore the main
                part of the subject. The integrator works according to the free-path sampling method. </p>

            <p>
                In a first step, I started by implementing the phasefunction. Since I already had 60 pts of features, I
                implemented the isotropic phase function. It was in the end nothing really complicated. A toString
                function and a sample function that returns a pdf while filling a data structure a data structure for
                rendering. I created the file phasefunction.cpp for this. </p>

            <p>
                The second part implemented is the medium part. In my case I implement a homogeneous medium. The
                homogeneous part is easier than the heterogeneous part. This has greatly facilitated the programming of
                the sample, tr and inv_tr methods. No need to use complicated methods. Just some
                calculations with exponentials and logarithms. The equations are a reproduction of the
                slides of the course. </p>

            <p>
                The implementation of the integrator is quite similar to path mis. The difference is that you have to
                attenuate the ray by the medium and on the other hand break the path of the ray in the middle of the
                ray in order to give this scattering effect. For this we use the helpers functions of the medium and
                phase function within the integrator. The rest of the logic is otherwise similar to the
                path_mis. </p>

            <h3> Validation </h3>


            <div class="twentytwenty-container">
                <img src="../scenes/project/disney/cbox_path_mis_specular_no_rough.png" alt="Scene with path mis" class="img-responsive">
                <img src="../scenes/project/volumetric/volumetric_no_scatter.png"
                    alt="Scene with volumetric and no scattering" class="img-responsive">
            </div> <br>

            <div class="twentytwenty-container">
                <img src="../scenes/project/disney/cbox_path_mis_specular_no_rough.png" alt="Scene with path mis" class="img-responsive">
                <img src="../scenes/project/volumetric/volumetric.png"
                    alt="Scene with volumetric path (0.15 attenuation, 0.15 scattering)" class="img-responsive">
            </div> <br>

            <div class="twentytwenty-container">
                <img src="../scenes/project/disney/cbox_path_mis_specular_no_rough.png" alt="Scene with path mis" class="img-responsive">
                <img src="../scenes/project/volumetric/volumetric_with_bb.png"
                    alt="Scene with volumetric and bounding box" class="img-responsive">
            </div> <br>

            <!-- ================================================================= -->

            <h2>Windowed sync filter</h2>

            <p>
                <strong>Files Changed:</strong>
            <ul>
                <li><code>rfilter.cpp</code></li>
            </ul>

            <h3>Description</h3>

            <p>
                The part with the reconstruction filter does not imply big modifications in the raytracer
                nori. There is a file rfilter.cpp with different reconstruction filters. We see for example that
                for example that there is an implementation of the box filter or the gaussian filter. In my case I have
                decided to implement a windowed sync filter.</p>
            <p>
                As for the implementation itself, I chose to use the code given in PBR books.
                The implementation is not very difficult. You just have to extend the reconstruction class filter
                class and then add the corresponding code.
            </p>

            <h3> Technical details : </h3>

            <p> I created the new class WindowedSyncFilter in the file rfilter.cpp. This class extends the class
                ReconstructionFilter. We have in this class two parameters, namely radius and tau. For the eval
                function, I created the sinc function and use it inside of the eval function.</p>

            <h3> Validation </h3>

            <div class="twentytwenty-container">
                <img src="../scenes/pa4/cbox/cbox_path_mis.png" alt="Basic reconstruction filter"
                    class="img-responsive">
                <img src="../scenes/project/windowed sync filter/cbox_path_mis.png" alt="Windowed reconstruction filter"
                    class="img-responsive">
            </div> <br>


            <div class="twentytwenty-container">
                <img src="../scenes/project/windowed sync filter/base_filter.png" alt="Base reconstruction filter"
                    class="img-responsive">
                <img src="../scenes/project/windowed sync filter/windowed.png" alt="Windowed reconstruction filter"
                    class="img-responsive">
            </div> <br>

            <!-- ================================================================= -->

            <h2>Rendering on the euler cluster</h2>

            <p>
                <strong>Files Changed:</strong>
            <ul>
                <li><code>main_euler.cpp</code></li>
                <li><code>render.cpp</code></li>
            </ul>

            <h3>Description</h3>

            <p>
                To render on the euler cluster, the first step is to connect to the cluster, clone the project code and
                compile it normally with cmake.
                compile it normally with cmake. This is where the only difficulty of this part comes in. Nori is
                compiled with a
                graphical interface, namely the nanogui library. In order to be able to generate images on euler, you
                have to compile nori without this library.
                You have to compile nori without this graphical interface. </p>
            <p>
                To do this, I have generated a second program nori_euler which works without the graphical library.
                There are
                some difficulties though. The rendering is multi-threaded. The main thread does not wait for the
                the end of the other threads. This implies that we had to find a mechanism to synchronize the threads
                between them. My solution works as follows. In the main loop, we have a while loop that tests every
                second and exits once the rendering is complete.
                In the main loop, we have a while loop that tests every second and exits when the rendering is finished.
                It is very important to use the sleep function.
                Otherwise, the program will just loop and lose a lot of computing power to the other threads, which
                really need it.
                that really need computing power.
            </p>

            <h3> Technical details : </h3>

            <p> I created for this features the file main_euler.cpp. In the main function I directly create a
                renderThread and render into this renderThread. We must give the path to the xml file in the program
                arguments. When the rendering starts, I wait until the rendering is done. We sleep one second and read
                the status of the rendering again. When the rendering is done, we can print it and return 0. </p>

            <p>
                There is still another step to do. We need to modify the cmake file and create two programs. The first
                one is the normal nori with a GUI. The second program is the nori for the render cluster without GUI.
                There are two main files. The first is main.cpp. The second is main_euler.cpp.
            </p>

            <h3> Validation </h3>

            <div class="twentytwenty-container">
                <img src="../scenes/project/euler/euler.png" alt="Euler rendering" class="img-responsive">
            </div> <br>

            <div class="twentytwenty-container">
                <img src="../scenes/pa4/cbox/cbox_path_mis.png" alt="Generated with nori" class="img-responsive">
                <img src="../scenes/project/euler/file.png" alt="Generated on the euler cluster" class="img-responsive">
            </div> <br>

            <!-- ================================================================= -->

            <h2>Advanced Camera Model</h2>

            <h3>Description</h3>

            <p>
                <strong>Files Changed:</strong>
            <ul>
                <li><code>advancedCamera.cpp</code></li>
                <li><code>render.cpp</code></li>
                <li><code>common.h</code></li>
            </ul>

            <strong>Idea:</strong> Augment the basic perspective camera model in order to achieve three additional
            effects:
            <ol>
                <li>Depth of Field using the thin lens model.</li>
                <li>Barrel distortion using the same method as the mitsuba renderer.</li>
                <li>Chromatic Aberrations by sampling color channels individually.</li>
            </ol>
            </p>
            <p>
                All three of these effects were implemented onto a new camera class called
                <code>AdvancedCamera : Camera</code>.
                We will now detail how the three different effects were achieved.
            </p>

            <h3> Depth of Field </h3>

            <p> In order to achieve the depth of field effect, the initial perspective camera was extended to use the
                <strong>Thin Lens Model</strong>
                as described in <a
                    href="https://www.pbr-book.org/3ed-2018/Camera_Models/Projective_Camera_Models#TheThinLensModelandDepthofField">PBRT§6.2.3</a>.
                The idea behind this is to simulate an infinitely thin lens that is defined by:
            <ol>
                <li><code>lensRadius</code>: The size of the lens. This will affect how shallow the depth of field is.
                </li>
                <li><code>focalDistance</code>: The distance between the lens and the sensor. This will affect which
                    part of the scene is in focus.</li>
            </ol>
            Using these two new attributes, we can modify our sampling method such that, if the lens radius is non-zero,
            the rays are resampled on the surface of the lens
            creating a small circle of confusion that will depend on the radius of the lens and the focal distance.
            </p>

            <h4> Validation: DOF </h4>

            <p>
                In order to validate the depth of field effect, we start by comparing how the different parameters
                affect the image, followed by an on/off comparison of the effect.
            </p>
            <div class="twentytwenty-container">
                <img src="images/dof_only.png" alt="focalDist=5.7, lensRadius=0.35" class="img-responsive">
                <img src="images/dof_fd5_7_lr0_1.png" alt="focalDist=5.7, lensRadius=0.1" class="img-responsive">
                <img src="images/dof_fd5_7_lr0_9.png" alt="focalDist=5.7, lensRadius=0.9" class="img-responsive">
                <img src="images/dof_fd5_lr0_35.png" alt="focalDist=5, lensRadius=0.35" class="img-responsive">
            </div> <br>
            <div class="twentytwenty-container">
                <img src="images/cbox_ref.png" alt="No DOF" class="img-responsive">
                <img src="images/dof_only.png" alt="focalDist=5.7, lensRadius=0.35" class="img-responsive">
            </div> <br>

            <h3> Barrel Distortion </h3>

            <p> For this part, we based our implementation on a similar effect achieved in mitsuba.
                The main idea is simply to solve a distortion polynomial and move the near plane around accordingly for
                each sample.
                This allows us to achieve a nice distortion effect, as our rays direction will be modified to fit the
                curve of said polynomial.
                The polynomial can be controlled using the second and fourth order terms, which are passed as an
                argument through <code>m_distortion</code>.
            </p>

            <h4> Validation: Distortion </h4>
            <div class="twentytwenty-container">
                <img src="images/cbox_ref.png" alt="No distortion" class="img-responsive">
                <img src="images/distortion_only.png" alt="m_distortion = (3, 3)" class="img-responsive">
            </div> <br>

            <h3> Chromatic Abberation </h3>

            <p> As our final camera effect, we chose to implement chromatic abberation.
                To do this, we need to split our rendering such that each color component is rendered individually.
                Doing so enables a slight offset between each color. In order to seperate the color channels for the
                rendering,
                we needed to augment the code in <code>render.cpp</code> and add a channel parameter to the sampling
                method so that it sets all colors but the one color to 0 individually.
            </p>

            <h3> Validation </h3>

            <div class="twentytwenty-container">
                <img src="images/cbox_ref.png" alt="Perspective Camera" class="img-responsive">
                <img src="images/cbox_adv_cam.png" alt="Advanced Camera" class="img-responsive">
            </div> <br>

            <p> Note that all of the images where only rendered to about 15% due to time constraints. </p>

            <!-- ================================================================= -->

            <h2> Environment Map Emitter </h2>

            <h3>Description</h3>

            <p>
                <strong>Files Changed:</strong>
            <ul>
                <li><code>envmap.cpp</code></li>
                <li><code>common.h</code></li>
            </ul>

            <strong>Idea:</strong> Have a high dynamic range spherical image placed in the back of your scene (at t =
            inf) and consider is as a giant textured
            area emitter which is sampled when no object is hit before <code>t == ray.max_t</code>.
            </p>
            <p>
                Most of the environment map emitter was implemented based off of the following <a
                    href="https://web.cs.wpi.edu/~emmanuel/courses/cs563/S07/projects/envsample.pdf">reference
                    paper</a>.
                This contained pseudo code for the more technical aspects of the implementation, such as the
                <code>sample1D</code>, and <code>precompute1D</code> methods, and were thus followed quite closely.
                The remainder of the implementation also follows the paper quite closely, with additional parameters
                added for artistic control.
            </p>

            <h3> Validation </h3>

            <p>
                Our validation is done by first showing a scene with a really high resolution environment map, followed
                by a comparison with mitsuba using the same scene and a lower resolution environment map.
                The environment maps are both from the following <a
                    href="https://polyhaven.com/a/chinese_garden">website</a>.
            </p>
            <div class="twentytwenty-container">
                <img src="images/envmap_chin.png" alt="High Resolution EnvMap" class="img-responsive">
            </div> <br>

            <div class="twentytwenty-container">
                <img src="images/envmap_nori.png" alt="Environment Map Nori" class="img-responsive">
                <img src="images/envmap_mitsuba.png" alt="Environment Map Mitsuba" class="img-responsive">
            </div> <br>

            <!-- ================================================================= -->

            <h2> Image Textures </h2>

            <h3>Description</h3>

            <p>
                <strong>Files Changed:</strong>
            <ul>
                <li><code>imagetexture.cpp</code></li>
                <li><code>common.h</code></li>
            </ul>

            <strong>Idea:</strong> Use a <code>.png</code> or <code>.jpg</code> file for a texture. Thus, given a uv
            coordinate pair, sample the related point on the image, while using billinear interpolation to reduce
            aliasing.
            </p>
            <p>
                To implement this feature, a new type of texture was created named
                <code>ImageTexture : Texture&#60;Color3f&#62;</code>.
                This texture type contains a set of parameters used to handle image file loading:
            <ul>
                <li><code>m_filename</code>: A relative path to the file, from the location of the scene xml.</li>
                <li><code>m_wrap</code>: How the texture will be handled at the image's edges (repeat or clamp).</li>
                <li><code>m_data</code>: The loaded-in bytes from the image file.</li>
                <li><code>m_width</code>: The width of the image.</li>
                <li><code>m_height</code>: The height of the image.</li>
                <li><code>m_channels</code>: The number of color channels the image contains (3 for rgb, 4 for rgba).
                </li>
            </ul>
            The image is first loaded into the <code>m_data</code> field using the <code>stbi_load</code> function.
            During evaluation, the uv coordinates are converted into byte indexes for the data map, which are then used
            to retrieve the color channels individually.
            Finally, the neighboring pixels are sample and billinear interpolation is performed, in order to reduce the
            texture aliasing.
            </p>

            <h3> Validation </h3>

            <div class="twentytwenty-container">
                <img src="images/cbox_ref.png" alt="No Textures" class="img-responsive">
                <img src="images/texture.png" alt="With Textures (wrap = Repeat)" class="img-responsive">
                <img src="images/texture_clamp.png" alt="With Textures (wrap = Clamp)" class="img-responsive">
            </div> <br>

            <!-- ================================================================= -->

            <h2> Normal Map </h2>

            <h3>Description</h3>

            <p>
                <strong>Files Changed:</strong>
            <ul>
                <li><code>normalmap.cpp</code></li>
                <li><code>mesh.cpp</code></li>
                <li><code>shape.h</code></li>
                <li><code>shape.cpp</code></li>
            </ul>

            <strong>Idea:</strong> Same idea as Image Textures, but this time use the rgb values of the sampled pixel to
            set the shading frame's normals.
            </p>
            <p>
                To implement this feature, a new type of texture was created named
                <code>NormalMap : Texture&#60;Normal3f&#62;</code>.
                A new <code>m_normalMap</code> field was added to the Shape class, and the <code>Shape::addChild</code>
                method was updated accordingly.
                The code for this part is almost identical to the code for the <code>ImageTexture</code> class, with the
                only difference being in when the <code>eval</code> method is called.
                For this feature, we need to check, inside of <code>Mesh::setHitInformation</code> if a normal map was
                loaded-in for the mesh.
                If so, then, using the intersection point's uv coordinates, we set <code>its.shFrame.n</code> to the
                sampled rgb value of the texture.
            </p>

            <h3> Validation </h3>
            <p>To validate our normal maps, we will compare our cbox scene without normal maps, with one that has normal
                maps.
                Normal maps will be more visible on flat surfaces, so the effect is more present on the right wall than
                on the sphere.
            </p>
            <div class="twentytwenty-container">
                <img src="images/texture.png" alt="Only Textures" class="img-responsive">
                <img src="images/texture_normal.png" alt="With Normal Map" class="img-responsive">
            </div> <br>

            <!-- ================================================================= -->

            <h2> Modeling Meshes </h2>

            <h3>Description</h3>
            <p>
                In order to achieve our final scene, we decided to model some custom meshes.
                One of the models in question is a tree model, which was sculped and modeled from scratch using blender.
                Here are some screen shots from the modeling process, starting with some basic sculpting, followed by
                some texturing and branch creation, before achieving our final model.
            </p>

            <div class="container">
                <img src="images/tree_wip1.png" alt="Early Tree trunk" class="img-responsive">
                <img src="images/tree_wip2.png" alt="Tree trunk with additional branches" class="img-responsive">
                <img src="images/tree_wip3.png" alt="Tree trunk with additional branches and roots"
                    class="img-responsive">
                <img src="images/tree_wip4.png" alt="Modeling a branch" class="img-responsive">
                <img src="images/tree_wip5.png" alt="Texturing" class="img-responsive">
                <img src="images/tree_wip7.png" alt="Final tree" class="img-responsive">
            </div> <br>

            <!-- ================================================================= -->

            <h2> Procedural Volumes </h2>

            <h3>Description</h3>

            <p>
                <strong>Files Changed:</strong>
            <ul>
                <li><code>perlinnoise.cpp</code></li>
            </ul>

            <strong>Idea:</strong> Create a procedural spherical shape by having a sphere whose radius is determined at
            every point by a 2D perlin noise function.
            </p>
            <p>
                To implement this feature, we creted a new volume called <code>PerlinSphere</code>. The implementation
                follows that of a regular sphere quite closely, with the added
                detail that, when testing for intersection, we first intersect with a sphere of fixed radius, then we
                use that intersection point to compute the height value
                from a perlin noise function and recompute the intersection using a radius that is scaled by the output
                of the noise function.
                This yields a wavy rough surface whose amplitude can be tweaked using the <code>height</code> xml
                parameter.
            </p>

            <h3> Validation </h3>
            <p>
            </p>
            <div class="twentytwenty-container">
                <img src="images/nonoise.png" alt="Normal" class="img-responsive">
                <img src="images/noise.png" alt="With noise" class="img-responsive">
            </div> <br>

            <!-- ================================================================= -->

            <h2> Emissive Participating Media </h2>

            <h3>Description</h3>

            <p>This feature was unfortunately not implemented due to time constraints.</p>

            <!-- ================================================================= -->

        </div>
    </div>



    <!-- Bootstrap core JavaScript -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="resources/bootstrap.min.js"></script>
    <!-- <script src="/js/offcanvas.js"></script> -->
    <script src="resources/jquery.event.move.js"></script>
    <script src="resources/jquery.twentytwenty.js"></script>


    <script>
        $(window).load(function () { $(".twentytwenty-container").twentytwenty({ default_offset_pct: 0.5 }); });
    </script>

</body>

</html>